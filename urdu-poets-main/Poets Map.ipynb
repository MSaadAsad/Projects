{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0629f6d",
   "metadata": {},
   "source": [
    "## Getting Poets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44e96ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b70fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a962df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_Urdu_poets\"\n",
    "\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91fafc72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n"
     ]
    }
   ],
   "source": [
    "if response.status_code == 200:\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find the element with id \"content\"\n",
    "    content = soup.find(id=\"content\")\n",
    "    \n",
    "    # Find all hyperlinks within the \"content\" element\n",
    "    hyperlinks = content.find_all(\"a\")\n",
    "    \n",
    "    # Define a regular expression pattern to match valid URLs\n",
    "    pattern = r\"^\\/wiki\\/[^.:]+$\"\n",
    "    \n",
    "    # Create a list of tuples containing poet names and their URLs\n",
    "    poet_names_and_urls = [(link.text, link[\"href\"]) for link in hyperlinks if link.has_attr(\"href\") and re.match(pattern, link[\"href\"])]\n",
    "    \n",
    "    # Extract poet names from the list of tuples\n",
    "    poet_names = [poet[0] for poet in poet_names_and_urls]\n",
    "    \n",
    "    # Find the indices of \"Amir Khusro\" and \"Fuzail Ahmad Nasiri\"\n",
    "    min_index = poet_names.index(\"Amir Khusro\")\n",
    "    max_index = poet_names.index(\"Fuzail Ahmad Nasiri\")\n",
    "    \n",
    "    # Filter the list of poet names to include only those within the desired range\n",
    "    filtered_poets = poet_names[min_index:max_index + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "260404d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wikitext(title):\n",
    "    # Base URL for Wikipedia API\n",
    "    base_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    \n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"titles\": title,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the Wikipedia API\n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    # Parse the response content as JSON\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract the page ID from the response\n",
    "    page_id = next(iter(data['query']['pages']))\n",
    "    \n",
    "    # Get the content of the first revision of the page\n",
    "    wikitext = data['query']['pages'][page_id].get('revisions', [{}])[0].get('*', '')\n",
    "    \n",
    "    return wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fa8c02f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_data = []  # To store the extracted data for all poets\n",
    "base_url = \"https://en.wikipedia.org\"  # Base URL for Wikipedia\n",
    "\n",
    "for poet in filtered_poets:  # Loop through each filtered poet\n",
    "\n",
    "    title = poet\n",
    "    wikitext = fetch_wikitext(title)  # Fetch wikitext for the poet's page\n",
    "\n",
    "    redirect_match = re.match(r'#REDIRECT \\[\\[(.*?)\\]\\]', wikitext)\n",
    "    if redirect_match:\n",
    "        new_title = redirect_match.group(1).replace(' ', '_')\n",
    "        wikitext = fetch_wikitext(new_title)  # Follow redirects if present\n",
    "    \n",
    "    # Extract birth and death years using regular expressions\n",
    "    birth_date_pattern = r'birth_date\\s*=\\s*(?:.*?)(\\d{4})'\n",
    "    death_date_pattern = r'death_date\\s*=\\s*(?:.*?)(\\d{4})'\n",
    "    \n",
    "    birth_match = re.search(birth_date_pattern, wikitext)\n",
    "    birth_year = birth_match.group(1) if birth_match else None\n",
    "    \n",
    "    death_match = re.search(death_date_pattern, wikitext)\n",
    "    death_year = death_match.group(1) if death_match else None\n",
    "\n",
    "    # Extract birth and death places using regular expressions\n",
    "    birth_place_match = re.search(r'birth_place\\s*=\\s*(\\[\\[.*?\\]\\])', wikitext, re.DOTALL)\n",
    "    death_place_match = re.search(r'death_place\\s*=\\s*(\\[\\[.*?\\]\\])', wikitext, re.DOTALL)\n",
    "\n",
    "    birth_place = ' '.join(re.findall(r'\\[\\[(.*?)\\]\\]', birth_place_match.group(1))) if birth_place_match else None\n",
    "    death_place = ' '.join(re.findall(r'\\[\\[(.*?)\\]\\]', death_place_match.group(1))) if death_place_match else None\n",
    "\n",
    "    # Append the extracted data to the 'all_data' list as a dictionary\n",
    "    all_data.append({\n",
    "        'Name': poet,\n",
    "        'Birthplace': birth_place,\n",
    "        'Death Place': death_place,\n",
    "        'Birth Year': birth_year,\n",
    "        'Death Year': death_year\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "867a9d4a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "849c39a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6086b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('poets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "aba9d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "081976bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='poets.csv' target='_blank'>poets.csv</a><br>"
      ],
      "text/plain": [
       "/Users/muhammadsaadasad/poets.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(FileLink(\"poets.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a54487",
   "metadata": {},
   "source": [
    "Manual processing of incomplete data fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c1c7d7",
   "metadata": {},
   "source": [
    "## Location Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d03dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/muhammadsaadasad/Downloads/Poets-fix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4512bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39bcd39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location_info(place):\n",
    "    # Base URL for Google Geocoding API\n",
    "    base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "    \n",
    "    # Send a GET request to the API with the specified place and API key\n",
    "    response = requests.get(base_url, params={\"address\": place, \"key\": api_key})\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    res = response.json()\n",
    "\n",
    "    if res['status'] == 'OK':\n",
    "        # Extract latitude and longitude from the response\n",
    "        lat = res['results'][0]['geometry']['location']['lat']\n",
    "        lng = res['results'][0]['geometry']['location']['lng']\n",
    "        \n",
    "        # Find the state (administrative_area_level_1) from the address components\n",
    "        for component in res['results'][0]['address_components']:\n",
    "            if 'administrative_area_level_1' in component['types']:\n",
    "                state = component['long_name']\n",
    "                break\n",
    "        else:\n",
    "            state = None  # If state not found\n",
    "        \n",
    "        return lat, lng, state  # Return latitude, longitude, and state\n",
    "    else:\n",
    "        return None, None, None  # Return None if status is not OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af40f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['B_Lat'], df['B_Lon'], df['B_State'] = zip(*df['Birthplace'].apply(lambda x: get_location_info(x)))\n",
    "df['D_Lat'], df['D_Lon'], df['D_State'] = zip(*df['Death Place'].apply(lambda x: get_location_info(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99d3a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Poets-Clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11fa1d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='Poets-Clean.csv' target='_blank'>Poets-Clean.csv</a><br>"
      ],
      "text/plain": [
       "/Users/muhammadsaadasad/Poets-Clean.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(FileLink(\"Poets-Clean.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f2074b",
   "metadata": {},
   "source": [
    "## Plotting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f0b20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import Image, display\n",
    "import cv2\n",
    "import os\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956bfccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/muhammadsaadasad/Downloads/Poets-Clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cb3b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [[1200, 1300], [1300, 1400], [1400, 1500], [1500, 1600], [1600, 1700]]\n",
    "titles = [\"1200s\", \"1300s\", \"1400s\", \"1500s\", \"1600s\"]\n",
    "\n",
    "for i in range(170, 200):\n",
    "    years.append([(i*10) - 15, (i*10) + 20])\n",
    "    titles.append(str(i*10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fbf932",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = [\n",
    "    \"Proto-Urdu emerges; Amir Khusro blends Persian, Turkic, and Indian linguistic elements.\",\n",
    "    \"Khusro popularizes qawwali and ghazal styles, laying foundations for Urdu's poetic traditions.\",\n",
    "    \"The Deccan region starts embracing early forms of Urdu called Dakkani.\",\n",
    "    \"Mughal influence promotes Persian, but local interactions birth Urdu's precursors. Dakkani poetry thrives in the south.\",\n",
    "    \"Urdu crystallizes in North India. Delhi becomes a hub, and Wali Deccani bridges southern and northern Urdu traditions.\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"Decline of the Mughal Empire & Nadir Shah's invasion (1739).\", #1730s\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"Shift of Urdu cultural center from Delhi to Lucknow.\", #1770s\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"British colonial expansion.\", #1810\n",
    "    \"\",\n",
    "    \"English becomes the language of administration, and Urdu emerges as a lingua franca for North Indians.\" #1830\n",
    "    \"\",\n",
    "    \"British annexation of Awadh (1856).\", #1850\n",
    "    \"Decline of Lucknow as centre of Urdu\", #1860\n",
    "    \"Establishment of Aligarh Muslim University (1875).\",\n",
    "    \"Aligarh's rise as a beacon of Urdu literature and thought.\",\n",
    "    \"\",\n",
    "    \"Lahore's burgeoning cultural activities.\",\n",
    "    \"Lahore joins the ranks as a significant hub for Urdu literature and thought.\", #1910\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"Partition of India: Hindi is made the Official Language of India while Urdu becomes the Pakistani Lingua Franca.\",\n",
    "    \"With the creation of Pakistan, Lahore and Karachi become central to Urdu's cultural and literary life.\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"Global migrations.\",\n",
    "    \"Urdu diaspora grows in the West, infusing traditional poetry with contemporary experiences.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd099b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dict = dict(zip(titles, captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a73c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Death Year'] = pd.to_numeric(df['Death Year'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e513891",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5967a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, gap in enumerate(years):\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2, \n",
    "                    subplot_titles=('Births', 'Deaths'),\n",
    "                    specs=[[{'type': 'densitymapbox'}, {'type': 'densitymapbox'}]])\n",
    "    \n",
    "    year_range = (gap[0], gap[1])\n",
    "    \n",
    "    filtered_df_b = df[(df['Birth Year'] >= year_range[0]) & (df['Birth Year'] <= year_range[1])]\n",
    "    agg_df_b = filtered_df_b.groupby(['B_Lat', 'B_Lon']).size().reset_index(name='Frequency')\n",
    "    \n",
    "    filtered_df_d = df[(df['Death Year'] >= year_range[0]) & (df['Death Year'] <= year_range[1])]\n",
    "    agg_df_d = filtered_df_d.groupby(['D_Lat', 'D_Lon']).size().reset_index(name='Frequency')\n",
    "    \n",
    "    heatmap_births = px.density_mapbox(agg_df_b, \n",
    "                        lat='B_Lat', \n",
    "                        lon='B_Lon', \n",
    "                        z='Frequency', \n",
    "                        radius=10,\n",
    "                        mapbox_style=\"light\").data[0]  # Simpler background)\n",
    "\n",
    "    heatmap_deaths = px.density_mapbox(agg_df_d, \n",
    "                        lat='D_Lat', \n",
    "                        lon='D_Lon', \n",
    "                        z='Frequency', \n",
    "                        radius=10,\n",
    "                        mapbox_style=\"light\").data[0]  # Simpler background)\n",
    "    \n",
    "    fig.add_trace(heatmap_births, row=1, col=1)\n",
    "    fig.add_trace(heatmap_deaths, row=1, col=2)\n",
    "    \n",
    "    # Add a caption with the \"year\" variable\n",
    "    year = titles[i]\n",
    "    fig.add_annotation(text=year,  # Bold year\n",
    "                       xref=\"paper\", yref=\"paper\",\n",
    "                       x=0.5, y=1.15,  # Position of the year\n",
    "                       showarrow=False,\n",
    "                       font=dict(size=23))\n",
    "    \n",
    "    fig.add_annotation(text=merged_dict[year],  # Caption\n",
    "                       xref=\"paper\", yref=\"paper\",\n",
    "                       x=0.5, y=-0.1,  # Position of the caption\n",
    "                       showarrow=False,\n",
    "                       font=dict(size=10),\n",
    "                       bgcolor=\"rgba(255, 255, 255, 0.7)\",\n",
    "                       bordercolor=\"black\",\n",
    "                       borderwidth=1,\n",
    "                       borderpad=4)\n",
    "    \n",
    "    fig.update_layout(coloraxis_showscale=False)\n",
    "    fig.update_layout(mapbox1=dict(accesstoken=token, center=dict(lat=26, lon=79), zoom=2.75),\n",
    "                      mapbox2=dict(accesstoken=token, center=dict(lat=26, lon=79), zoom=2.75))\n",
    "    image_filename = f\"map_{titles[i]}.png\"\n",
    "    fig.write_image(image_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b22ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = cv2.VideoWriter(downloads_path, cv2.VideoWriter_fourcc(*'mp4v'), 0.5, size)\n",
    "\n",
    "img_files = sorted([img for img in os.listdir() if img.startswith(\"map_\") and img.endswith(\".png\")])\n",
    "frame = cv2.imread(img_files[0])\n",
    "h, w, layers = frame.shape\n",
    "size = (w,h)\n",
    "\n",
    "for i in range(len(img_files)):\n",
    "    img = cv2.imread(img_files[i])\n",
    "    out.write(img)\n",
    "\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
